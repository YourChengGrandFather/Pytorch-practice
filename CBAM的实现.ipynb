{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc384786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2543dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # 利用1x1卷积代替全连接\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('x',x.shape)\n",
    "        print('avg_pool', self.avg_pool(x).shape)\n",
    "        print('max_pool', self.max_pool(x).shape)\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        print('out', out.shape)\n",
    "        print('result', self.sigmoid(out).shape)\n",
    "        return self.sigmoid(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adf1ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([2, 512, 28, 28])\n",
      "avg_pool torch.Size([2, 512, 1, 1])\n",
      "max_pool torch.Size([2, 512, 1, 1])\n",
      "out torch.Size([2, 512, 1, 1])\n",
      "result torch.Size([2, 512, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3882]],\n",
       "\n",
       "         [[0.3906]],\n",
       "\n",
       "         [[0.5325]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.5554]],\n",
       "\n",
       "         [[0.5454]],\n",
       "\n",
       "         [[0.4556]]],\n",
       "\n",
       "\n",
       "        [[[0.3882]],\n",
       "\n",
       "         [[0.3906]],\n",
       "\n",
       "         [[0.5325]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.5554]],\n",
       "\n",
       "         [[0.5454]],\n",
       "\n",
       "         [[0.4556]]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,512,28,28)\n",
    "b,c,w,h = x.shape\n",
    "channelAttention = ChannelAttention(c)\n",
    "channelAttention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f90ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        print('x',x.shape)\n",
    "        print('avg_pool', avg_out.shape)\n",
    "        print('max_pool', avg_out.shape)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        print('cat_x', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        print('cat_x', x.shape)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d58ba9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([2, 512, 28, 28])\n",
      "avg_pool torch.Size([2, 1, 28, 28])\n",
      "max_pool torch.Size([2, 1, 28, 28])\n",
      "cat_x torch.Size([2, 2, 28, 28])\n",
      "cat_x torch.Size([2, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5344, 0.5581, 0.5547,  ..., 0.5775, 0.5661, 0.5444],\n",
       "          [0.5531, 0.5956, 0.5937,  ..., 0.6075, 0.5751, 0.5679],\n",
       "          [0.4813, 0.5607, 0.5571,  ..., 0.5881, 0.5915, 0.5816],\n",
       "          ...,\n",
       "          [0.4418, 0.5045, 0.5202,  ..., 0.6416, 0.6296, 0.6327],\n",
       "          [0.4493, 0.5051, 0.5097,  ..., 0.6030, 0.6108, 0.5946],\n",
       "          [0.4400, 0.4956, 0.5074,  ..., 0.6156, 0.6322, 0.6198]]],\n",
       "\n",
       "\n",
       "        [[[0.5344, 0.5581, 0.5547,  ..., 0.5775, 0.5661, 0.5444],\n",
       "          [0.5531, 0.5956, 0.5937,  ..., 0.6075, 0.5751, 0.5679],\n",
       "          [0.4813, 0.5607, 0.5571,  ..., 0.5881, 0.5915, 0.5816],\n",
       "          ...,\n",
       "          [0.4418, 0.5045, 0.5202,  ..., 0.6416, 0.6296, 0.6327],\n",
       "          [0.4493, 0.5051, 0.5097,  ..., 0.6030, 0.6108, 0.5946],\n",
       "          [0.4400, 0.4956, 0.5074,  ..., 0.6156, 0.6322, 0.6198]]]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,512,28,28)\n",
    "b,c,w,h = x.shape\n",
    "spatialAttention = SpatialAttention()\n",
    "spatialAttention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3c153ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cbam_block(nn.Module):\n",
    "    def __init__(self, channel, ratio=8, kernel_size=7):\n",
    "        super(Cbam_block, self).__init__()\n",
    "        self.channelattention = ChannelAttention(channel, ratio=ratio)\n",
    "        self.spatialattention = SpatialAttention(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channelattention(x)\n",
    "        print('xxx',x.shape)\n",
    "        x = x * self.spatialattention(x)\n",
    "        print('xxx2',x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "469885a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([2, 512, 28, 28])\n",
      "avg_pool torch.Size([2, 512, 1, 1])\n",
      "max_pool torch.Size([2, 512, 1, 1])\n",
      "out torch.Size([2, 512, 1, 1])\n",
      "result torch.Size([2, 512, 1, 1])\n",
      "xxx torch.Size([2, 512, 28, 28])\n",
      "x torch.Size([2, 512, 28, 28])\n",
      "avg_pool torch.Size([2, 1, 28, 28])\n",
      "max_pool torch.Size([2, 1, 28, 28])\n",
      "cat_x torch.Size([2, 2, 28, 28])\n",
      "cat_x torch.Size([2, 1, 28, 28])\n",
      "xxx2 torch.Size([2, 512, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2486, 0.2509, 0.2418,  ..., 0.2716, 0.2586, 0.2966],\n",
       "          [0.2231, 0.2237, 0.2301,  ..., 0.2742, 0.2688, 0.3155],\n",
       "          [0.2366, 0.2315, 0.2503,  ..., 0.2789, 0.2646, 0.3042],\n",
       "          ...,\n",
       "          [0.2475, 0.2372, 0.2675,  ..., 0.2878, 0.2696, 0.3045],\n",
       "          [0.2793, 0.2612, 0.2954,  ..., 0.3058, 0.2786, 0.3024],\n",
       "          [0.2898, 0.2695, 0.3027,  ..., 0.3031, 0.2861, 0.3004]],\n",
       "\n",
       "         [[0.2135, 0.2154, 0.2076,  ..., 0.2332, 0.2220, 0.2547],\n",
       "          [0.1915, 0.1921, 0.1975,  ..., 0.2354, 0.2308, 0.2709],\n",
       "          [0.2032, 0.1987, 0.2150,  ..., 0.2394, 0.2272, 0.2612],\n",
       "          ...,\n",
       "          [0.2126, 0.2037, 0.2297,  ..., 0.2471, 0.2315, 0.2615],\n",
       "          [0.2398, 0.2243, 0.2537,  ..., 0.2626, 0.2392, 0.2596],\n",
       "          [0.2488, 0.2314, 0.2599,  ..., 0.2602, 0.2457, 0.2579]],\n",
       "\n",
       "         [[0.2407, 0.2429, 0.2341,  ..., 0.2629, 0.2503, 0.2872],\n",
       "          [0.2160, 0.2166, 0.2227,  ..., 0.2654, 0.2603, 0.3054],\n",
       "          [0.2291, 0.2241, 0.2424,  ..., 0.2700, 0.2562, 0.2946],\n",
       "          ...,\n",
       "          [0.2397, 0.2296, 0.2590,  ..., 0.2786, 0.2610, 0.2948],\n",
       "          [0.2704, 0.2529, 0.2860,  ..., 0.2961, 0.2697, 0.2927],\n",
       "          [0.2806, 0.2609, 0.2931,  ..., 0.2934, 0.2770, 0.2908]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.2151, 0.2171, 0.2092,  ..., 0.2350, 0.2237, 0.2566],\n",
       "          [0.1930, 0.1935, 0.1990,  ..., 0.2372, 0.2326, 0.2730],\n",
       "          [0.2047, 0.2002, 0.2166,  ..., 0.2413, 0.2289, 0.2632],\n",
       "          ...,\n",
       "          [0.2142, 0.2052, 0.2315,  ..., 0.2490, 0.2333, 0.2635],\n",
       "          [0.2416, 0.2260, 0.2556,  ..., 0.2646, 0.2411, 0.2616],\n",
       "          [0.2507, 0.2332, 0.2619,  ..., 0.2622, 0.2475, 0.2599]],\n",
       "\n",
       "         [[0.2330, 0.2351, 0.2266,  ..., 0.2545, 0.2424, 0.2780],\n",
       "          [0.2091, 0.2097, 0.2156,  ..., 0.2570, 0.2520, 0.2957],\n",
       "          [0.2218, 0.2169, 0.2346,  ..., 0.2613, 0.2480, 0.2851],\n",
       "          ...,\n",
       "          [0.2320, 0.2223, 0.2508,  ..., 0.2697, 0.2527, 0.2854],\n",
       "          [0.2617, 0.2448, 0.2769,  ..., 0.2866, 0.2611, 0.2834],\n",
       "          [0.2716, 0.2526, 0.2837,  ..., 0.2840, 0.2681, 0.2815]],\n",
       "\n",
       "         [[0.2652, 0.2676, 0.2579,  ..., 0.2897, 0.2759, 0.3164],\n",
       "          [0.2380, 0.2386, 0.2454,  ..., 0.2925, 0.2868, 0.3366],\n",
       "          [0.2524, 0.2469, 0.2670,  ..., 0.2975, 0.2823, 0.3246],\n",
       "          ...,\n",
       "          [0.2641, 0.2530, 0.2854,  ..., 0.3070, 0.2876, 0.3249],\n",
       "          [0.2979, 0.2787, 0.3151,  ..., 0.3262, 0.2972, 0.3226],\n",
       "          [0.3091, 0.2875, 0.3230,  ..., 0.3233, 0.3052, 0.3204]]],\n",
       "\n",
       "\n",
       "        [[[0.2486, 0.2509, 0.2418,  ..., 0.2716, 0.2586, 0.2966],\n",
       "          [0.2231, 0.2237, 0.2301,  ..., 0.2742, 0.2688, 0.3155],\n",
       "          [0.2366, 0.2315, 0.2503,  ..., 0.2789, 0.2646, 0.3042],\n",
       "          ...,\n",
       "          [0.2475, 0.2372, 0.2675,  ..., 0.2878, 0.2696, 0.3045],\n",
       "          [0.2793, 0.2612, 0.2954,  ..., 0.3058, 0.2786, 0.3024],\n",
       "          [0.2898, 0.2695, 0.3027,  ..., 0.3031, 0.2861, 0.3004]],\n",
       "\n",
       "         [[0.2135, 0.2154, 0.2076,  ..., 0.2332, 0.2220, 0.2547],\n",
       "          [0.1915, 0.1921, 0.1975,  ..., 0.2354, 0.2308, 0.2709],\n",
       "          [0.2032, 0.1987, 0.2150,  ..., 0.2394, 0.2272, 0.2612],\n",
       "          ...,\n",
       "          [0.2126, 0.2037, 0.2297,  ..., 0.2471, 0.2315, 0.2615],\n",
       "          [0.2398, 0.2243, 0.2537,  ..., 0.2626, 0.2392, 0.2596],\n",
       "          [0.2488, 0.2314, 0.2599,  ..., 0.2602, 0.2457, 0.2579]],\n",
       "\n",
       "         [[0.2407, 0.2429, 0.2341,  ..., 0.2629, 0.2503, 0.2872],\n",
       "          [0.2160, 0.2166, 0.2227,  ..., 0.2654, 0.2603, 0.3054],\n",
       "          [0.2291, 0.2241, 0.2424,  ..., 0.2700, 0.2562, 0.2946],\n",
       "          ...,\n",
       "          [0.2397, 0.2296, 0.2590,  ..., 0.2786, 0.2610, 0.2948],\n",
       "          [0.2704, 0.2529, 0.2860,  ..., 0.2961, 0.2697, 0.2927],\n",
       "          [0.2806, 0.2609, 0.2931,  ..., 0.2934, 0.2770, 0.2908]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.2151, 0.2171, 0.2092,  ..., 0.2350, 0.2237, 0.2566],\n",
       "          [0.1930, 0.1935, 0.1990,  ..., 0.2372, 0.2326, 0.2730],\n",
       "          [0.2047, 0.2002, 0.2166,  ..., 0.2413, 0.2289, 0.2632],\n",
       "          ...,\n",
       "          [0.2142, 0.2052, 0.2315,  ..., 0.2490, 0.2333, 0.2635],\n",
       "          [0.2416, 0.2260, 0.2556,  ..., 0.2646, 0.2411, 0.2616],\n",
       "          [0.2507, 0.2332, 0.2619,  ..., 0.2622, 0.2475, 0.2599]],\n",
       "\n",
       "         [[0.2330, 0.2351, 0.2266,  ..., 0.2545, 0.2424, 0.2780],\n",
       "          [0.2091, 0.2097, 0.2156,  ..., 0.2570, 0.2520, 0.2957],\n",
       "          [0.2218, 0.2169, 0.2346,  ..., 0.2613, 0.2480, 0.2851],\n",
       "          ...,\n",
       "          [0.2320, 0.2223, 0.2508,  ..., 0.2697, 0.2527, 0.2854],\n",
       "          [0.2617, 0.2448, 0.2769,  ..., 0.2866, 0.2611, 0.2834],\n",
       "          [0.2716, 0.2526, 0.2837,  ..., 0.2840, 0.2681, 0.2815]],\n",
       "\n",
       "         [[0.2652, 0.2676, 0.2579,  ..., 0.2897, 0.2759, 0.3164],\n",
       "          [0.2380, 0.2386, 0.2454,  ..., 0.2925, 0.2868, 0.3366],\n",
       "          [0.2524, 0.2469, 0.2670,  ..., 0.2975, 0.2823, 0.3246],\n",
       "          ...,\n",
       "          [0.2641, 0.2530, 0.2854,  ..., 0.3070, 0.2876, 0.3249],\n",
       "          [0.2979, 0.2787, 0.3151,  ..., 0.3262, 0.2972, 0.3226],\n",
       "          [0.3091, 0.2875, 0.3230,  ..., 0.3233, 0.3052, 0.3204]]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,512,28,28)\n",
    "b,c,w,h = x.shape\n",
    "cbam_block = Cbam_block(c)\n",
    "cbam_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2527df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n",
      "torch.Size([1, 1, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[2., 8., 6.],\n",
       "          [2., 8., 6.],\n",
       "          [2., 8., 6.]]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[[[1,2,3],[1,2,3],[1,2,3]]]])\n",
    "print(a.shape)\n",
    "b = torch.Tensor([[[[2,4,2]]]])\n",
    "print(b.shape)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6373a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
